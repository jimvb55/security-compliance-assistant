"""Mock LLM provider for the Security Compliance Assistant."""
import json
import re
from typing import Dict, List, Optional, Union, Any

from app.config import settings
from app.services.providers.base import BaseLLMProvider


class MockLLMProvider(BaseLLMProvider):
    """Mock LLM provider for testing and development."""
    
    def __init__(self):
        """Initialize the mock LLM provider."""
        pass
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> str:
        """Generate text from the mock LLM.
        
        Args:
            prompt: User prompt or question.
            system_prompt: System prompt to guide the LLM.
            temperature: Temperature for sampling (higher = more random).
            max_tokens: Maximum number of tokens to generate.
            
        Returns:
            Generated text.
        """
        # Simplified response based on prompt keywords
        prompt_lower = prompt.lower()
        
        if "hello" in prompt_lower or "hi" in prompt_lower:
            return "Hello! I'm a mock LLM provider for the Security Compliance Assistant. How can I help you today?"
        
        if "security" in prompt_lower:
            return "Security is an important aspect of any system. It involves protecting systems, networks, and data from unauthorized access or attacks."
        
        if "compliance" in prompt_lower:
            return "Compliance refers to adhering to regulations, standards, and legal requirements applicable to an organization. Common compliance frameworks include GDPR, HIPAA, SOC 2, ISO 27001, and others."
        
        if "vendor" in prompt_lower and "questionnaire" in prompt_lower:
            return "Vendor security questionnaires are tools used to assess the security posture of vendors or service providers. They help organizations understand the risks associated with sharing data or services with external parties."
        
        # Default response
        return f"This is a mock response to your query: '{prompt}'. In a real implementation, this would be generated by an actual LLM."
    
    def generate_with_sources(
        self,
        prompt: str,
        context: List[Dict],
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict:
        """Generate text with source citations.
        
        Args:
            prompt: User prompt or question.
            context: List of context items with text and metadata.
            system_prompt: System prompt to guide the LLM.
            temperature: Temperature for sampling (higher = more random).
            max_tokens: Maximum number of tokens to generate.
            
        Returns:
            Dictionary with 'answer', 'sources', and 'citations' fields.
        """
        # Create a simple answer that references the context
        context_texts = [item["text"] for item in context if "text" in item]
        context_sample = context_texts[:2] if context_texts else []
        
        # Create citations
        citations = {}
        sources = []
        
        for i, item in enumerate(context[:3], 1):
            if "metadata" in item:
                metadata = item["metadata"]
                doc_id = metadata.get("doc_id", f"doc_{i}")
                
                # Add citation
                citations[f"[{i}]"] = {
                    "doc_id": doc_id,
                    "filename": metadata.get("filename", "Unknown"),
                    "title": metadata.get("title", "Unknown"),
                }
                
                # Add source
                sources.append({
                    "text": item.get("text", "")[:100] + "...",
                    "doc_id": doc_id,
                    "filename": metadata.get("filename", "Unknown"),
                    "citation": f"[{i}]"
                })
        
        # Generate an answer with citations
        answer = f"Based on the provided documents, I can respond to your query about '{prompt}'.\n\n"
        
        if context_sample:
            answer += "The relevant information indicates that:"
            for i, text in enumerate(context_sample, 1):
                summary = text[:100] + "..." if len(text) > 100 else text
                answer += f"\n\n{summary} [{i}]"
        else:
            answer += "I don't have specific information about this query in the provided context."
        
        return {
            "answer": answer,
            "sources": sources,
            "citations": citations
        }
